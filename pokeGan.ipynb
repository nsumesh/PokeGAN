{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5647610,"sourceType":"datasetVersion","datasetId":3245742}],"dockerImageVersionId":30476,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"source":"<a href=\"https://www.kaggle.com/code/nikhilsumesh/ml-final-project?scriptVersionId=152884718\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>","metadata":{},"cell_type":"markdown"},{"cell_type":"markdown","source":"# ML Final Project\nNames: Rohit Reddy Goli, Nikhil Sumesh\n\n#### We will be creating a GAN to generate fake pokemon \nHere, we import the neccesary libraries","metadata":{}},{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, ConcatDataset\nfrom torchvision.datasets import ImageFolder\nimport torchvision.transforms as T\nfrom torchvision.utils import make_grid\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm\nimport random\nimport torch.nn as nn \nimport torch.nn.functional as F\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:17:59.78955Z","iopub.execute_input":"2023-11-29T14:17:59.790222Z","iopub.status.idle":"2023-11-29T14:18:03.484419Z","shell.execute_reply.started":"2023-11-29T14:17:59.790188Z","shell.execute_reply":"2023-11-29T14:18:03.483463Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Here, we will process the dataset of images and add new images of pokemon that have been recolored and mirrored/upside down. First, we set our directory and our image size for our tensors.","metadata":{}},{"cell_type":"code","source":"image_directory = \"/kaggle/input/mganset/mgan_dataset/mgan_dataset\"\nimage_size = 64\nbatch_size = 16\nnormalize_factor = (0.5,0.5,0.5),(0.5,0.5,0.5) #normalize data with mean and std","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:03.4861Z","iopub.execute_input":"2023-11-29T14:18:03.486624Z","iopub.status.idle":"2023-11-29T14:18:03.491596Z","shell.execute_reply.started":"2023-11-29T14:18:03.486596Z","shell.execute_reply":"2023-11-29T14:18:03.49056Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we will create our dataset from the given pokemon we will work with","metadata":{}},{"cell_type":"code","source":"normal_dataset = ImageFolder(image_directory,transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.ToTensor(),\n    T.Normalize(*normalize_factor)]))\ncolored_dataset = ImageFolder(image_directory,transform=T.Compose([\n    T.Resize(image_size),\n    T.CenterCrop(image_size),\n    T.ColorJitter(brightness=(1.5,2), hue=(-0.5,0.5)),\n    T.ToTensor(),\n    T.Normalize(*normalize_factor)]))\n\n#dataset = [normal_dataset, upside_dataset,mirror_dataset]\ndataset = [colored_dataset, normal_dataset]\ndataset = ConcatDataset(dataset)\ndataloader = DataLoader(dataset, batch_size, shuffle=True, num_workers=2)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:03.492555Z","iopub.execute_input":"2023-11-29T14:18:03.492844Z","iopub.status.idle":"2023-11-29T14:18:14.537091Z","shell.execute_reply.started":"2023-11-29T14:18:03.49282Z","shell.execute_reply":"2023-11-29T14:18:14.536313Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Since we normalized the images that we converted to tensors, we will now denormalize them for viewing purposes","metadata":{}},{"cell_type":"code","source":"def denormalize(image):\n    return image * normalize_factor[1][0] + normalize_factor[0][0]","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:14.539551Z","iopub.execute_input":"2023-11-29T14:18:14.539852Z","iopub.status.idle":"2023-11-29T14:18:14.544404Z","shell.execute_reply.started":"2023-11-29T14:18:14.539828Z","shell.execute_reply":"2023-11-29T14:18:14.54351Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now, we will display a batch of images which our GAN will use","metadata":{}},{"cell_type":"code","source":"def display_img(images, nmax=64):\n    fig, ax = plt.subplots(figsize = (32,32))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(make_grid(images.detach()[:nmax], nrow=16).permute(1,2,0))\n    plt.title(label=\"Pokemon Dataset Images\",)\n\ndef show(dataloader, nmax=64):\n    for images, _ in dataloader:\n        display_img(images,nmax)\n        test_image = images[random.randint(0, len(images)-1)]\n        print(test_image.shape)\n        break\n\n\nshow(dataloader)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:14.545404Z","iopub.execute_input":"2023-11-29T14:18:14.545658Z","iopub.status.idle":"2023-11-29T14:18:15.243615Z","shell.execute_reply.started":"2023-11-29T14:18:14.545637Z","shell.execute_reply":"2023-11-29T14:18:15.242661Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Discriminator Model below","metadata":{}},{"cell_type":"code","source":"disc_1 = nn.Sequential(\n    nn.Conv2d(3, 64, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(64),\n    nn.LeakyReLU(0.2, inplace=True),\n    \n    nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    \n    nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n    \n    nn.Conv2d(128, 128, kernel_size=4, stride=2, padding=1, bias=False),\n    nn.BatchNorm2d(128),\n    nn.LeakyReLU(0.2, inplace=True),\n\n    nn.Conv2d(128, 1, kernel_size=4, stride=2, padding=0, bias=False),\n    nn.Flatten(),\n    nn.Sigmoid())","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:15.245193Z","iopub.execute_input":"2023-11-29T14:18:15.245988Z","iopub.status.idle":"2023-11-29T14:18:15.267245Z","shell.execute_reply.started":"2023-11-29T14:18:15.24594Z","shell.execute_reply":"2023-11-29T14:18:15.266386Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"seed_size = 3\ngen_1 = nn.Sequential(\nnn.ConvTranspose2d(seed_size, 128, kernel_size=4, stride=2, padding=0, bias=False),\nnn.BatchNorm2d(128),\nnn.ReLU(),\n\nnn.ConvTranspose2d(128, 128, kernel_size=4, stride=2, padding = 1, bias=False),\nnn.BatchNorm2d(128),\nnn.ReLU(),\n\nnn.ConvTranspose2d(128, 128, kernel_size=4, stride = 2, padding=1, bias=False),\nnn.BatchNorm2d(128),\nnn.ReLU(),\n\nnn.ConvTranspose2d(128,64, kernel_size=4, stride=2, padding=1, bias=False),\nnn.BatchNorm2d(64),\nnn.ReLU(),\n\n\n\nnn.ConvTranspose2d(64, 3, kernel_size=4, stride=2, padding=1, bias=False),\nnn.Tanh()\n)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:15.268435Z","iopub.execute_input":"2023-11-29T14:18:15.268724Z","iopub.status.idle":"2023-11-29T14:18:15.284743Z","shell.execute_reply.started":"2023-11-29T14:18:15.268701Z","shell.execute_reply":"2023-11-29T14:18:15.283856Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### GPU Setup","metadata":{}},{"cell_type":"code","source":"def get_training_device():\n    # Use the GPU if possible\n    if torch.cuda.is_available():\n        return torch.device('cuda')\n    # Otherwise use the CPU :-(\n    return torch.device('cpu')\n\ndef to_device(data, device):\n    # This moves the tensors to the device (GPU, CPU)\n    if isinstance(data, (list,tuple)):\n        return [to_device(x, device) for x in data]\n    return data.to(device, non_blocking=True)\n\nclass DeviceDataLoader():\n    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n    def __init__(self, dataloader, device):\n        self.dataloader = dataloader\n        self.device = device\n        \n    def __iter__(self):\n        \"\"\"Yield a batch of data after moving it to device\"\"\"\n        for b in self.dataloader: \n            yield to_device(b, self.device)\n\n    def __len__(self):\n        \"\"\"Number of batches\"\"\"\n        return len(self.dataloader)\n\ndevice = get_training_device()\ndev_dataloader = DeviceDataLoader(dataloader, device)\ndevice","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:15.285904Z","iopub.execute_input":"2023-11-29T14:18:15.286223Z","iopub.status.idle":"2023-11-29T14:18:15.317331Z","shell.execute_reply.started":"2023-11-29T14:18:15.286193Z","shell.execute_reply":"2023-11-29T14:18:15.316381Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Training for Discriminator and Generator","metadata":{}},{"cell_type":"code","source":"generator = gen_1\ndiscriminator = disc_1\ndiscriminator = to_device(discriminator, device)\ngenerator = to_device(generator, device)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:15.31869Z","iopub.execute_input":"2023-11-29T14:18:15.319029Z","iopub.status.idle":"2023-11-29T14:18:18.331062Z","shell.execute_reply.started":"2023-11-29T14:18:15.318998Z","shell.execute_reply":"2023-11-29T14:18:18.330306Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_discriminator(dataset, optim):\n   \n    optim.zero_grad()\n    prediction = discriminator(dataset)\n    targets = torch.rand(dataset.size(0), 1) * (0.1-0) + 0\n    print(targets.size())\n    loss = F.binary_cross_entropy(prediction, targets)\n    score = torch.mean(prediction).item()\n\n    generated_pokemon = generator(dataset)\n    g_prediction = discriminator(generated_pokemon)\n    g_targets = torch.rand(generated_pokemon.size(0),1) * (1-0.9)+0.9\n    g_loss = F.binary_cross_entropy(g_prediction, g_targets)\n    g_score = torch.mean(g_prediction).item()\n\n    total_loss = loss + g_loss\n    total_loss.backward()\n    optim.step()\n    return total_loss.item(), score, g_score","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:18.332164Z","iopub.execute_input":"2023-11-29T14:18:18.332451Z","iopub.status.idle":"2023-11-29T14:18:18.339387Z","shell.execute_reply.started":"2023-11-29T14:18:18.332428Z","shell.execute_reply":"2023-11-29T14:18:18.338433Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def train_generator(optim):\n    discriminator = discriminator\n    optim.zero_grad()\n    latent_batch = torch.randn(batch_size, seed_size, 1,1)\n    generated_pokemon = generator(latent_batch)\n\n    prediction = discriminator(generated_pokemon)\n    targets = torch.zeros(generated_pokemon.size(0), 1)\n    loss = F.binary_cross_entropy(prediction, targets)\n\n    loss.backward()\n    optim.step()\n    return loss.item()","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:18.342381Z","iopub.execute_input":"2023-11-29T14:18:18.342663Z","iopub.status.idle":"2023-11-29T14:18:18.351064Z","shell.execute_reply.started":"2023-11-29T14:18:18.34264Z","shell.execute_reply":"2023-11-29T14:18:18.350346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nfrom torchvision.utils import save_image\n\nRESULTS_DIR = 'results'\nos.makedirs(RESULTS_DIR, exist_ok=True)\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:18.352169Z","iopub.execute_input":"2023-11-29T14:18:18.352502Z","iopub.status.idle":"2023-11-29T14:18:18.366047Z","shell.execute_reply.started":"2023-11-29T14:18:18.352472Z","shell.execute_reply":"2023-11-29T14:18:18.365269Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def save_results(index, latent_batch, show=True):\n    # Generate fake pokemon\n    \n    fake_pokemon = generator(latent_batch)\n    \n    # Make the filename for the output\n    fake_file = \"result-image-{0:0=4d}.png\".format(index)\n    \n    # Save the image\n    save_image(denormalize(fake_pokemon), os.path.join(RESULTS_DIR, fake_file), nrow=8)\n    print(\"Result Saved!\")\n    \n    if show:\n        fig, ax = plt.subplots(figsize=(8, 8))\n        ax.set_xticks([]); ax.set_yticks([])\n        ax.imshow(make_grid(fake_pokemon.cpu().detach(), nrow=8).permute(1, 2, 0))\n","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:18.367134Z","iopub.execute_input":"2023-11-29T14:18:18.367598Z","iopub.status.idle":"2023-11-29T14:18:18.377765Z","shell.execute_reply.started":"2023-11-29T14:18:18.367574Z","shell.execute_reply":"2023-11-29T14:18:18.376861Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Full Training Loop","metadata":{}},{"cell_type":"code","source":"def train(epochs, learning_rate, start_idx=1):\n    # Empty the GPU cache to save some memory\n    fixed_latent_batch = torch.randn(64, seed_size, 1, 1, device=device)\n    \n    torch.cuda.empty_cache()\n    # Track losses and scores\n    disc_losses = []\n    disc_scores = []\n    gen_losses = []\n    gen_scores = []\n    \n    # Create the optimizers\n    disc_optimizer = torch.optim.Adam(discriminator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n    gen_optimizer = torch.optim.Adam(generator.parameters(), lr=learning_rate, betas=(0.5, 0.9))\n    \n    # Run the loop\n    for epoch in range(epochs):\n        # Go through each image\n        for real_img, _ in tqdm(dev_dataloader):\n            # Train the discriminator\n            disc_loss, real_score, gen_score = train_discriminator(real_img, disc_optimizer)\n\n            gen_loss = train_generator(real_img, gen_optimizer)\n        disc_losses.append(disc_loss)\n        disc_scores.append(real_score)\n        gen_scores.append(gen_score)\n        gen_losses.append(gen_loss)\n\n        print(\"Epoch [{}/{}], gen_loss: {:.4f}, disc_loss: {:.4f}, real_score: {:.4f}, gen_score: {:.4f}\".format(\n            epoch+start_idx, epochs, gen_loss, disc_loss, real_score, gen_score))\n        \n        # Save the images and show the progress\n        save_results(epoch + start_idx, fixed_latent_batch, show=False)\n    \n    # Return stats\n    return disc_losses, disc_scores, gen_losses, gen_scores","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:18.378829Z","iopub.execute_input":"2023-11-29T14:18:18.379082Z","iopub.status.idle":"2023-11-29T14:18:18.38991Z","shell.execute_reply.started":"2023-11-29T14:18:18.379061Z","shell.execute_reply":"2023-11-29T14:18:18.389115Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = get_training_device()\ndevice\n\ndef debug_memory():\n    import collections, gc, resource, torch\n    print('maxrss = {}'.format(\n        resource.getrusage(resource.RUSAGE_SELF).ru_maxrss))\n    tensors = collections.Counter((str(o.device), o.dtype, tuple(o.shape))\n                                  for o in gc.get_objects()\n                                  if torch.is_tensor(o))\n    for line in tensors.items():\n        print('{}\\t{}'.format(*line))\n\nmem_debug = False\nif mem_debug:\n    debug_memory()\n\n# Clean up everything\ncleanup = False\nif cleanup:\n    import gc\n    del dev_dataloader\n    del discriminator\n    del generator\n    dev_dataloader = None\n    discriminator = None\n    generator = None\n    gc.collect()\n    torch.cuda.empty_cache()\n\n# Re-initialize the device dataloader\ndev_dataloader = DeviceDataLoader(dataloader, device)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:18.391207Z","iopub.execute_input":"2023-11-29T14:18:18.391502Z","iopub.status.idle":"2023-11-29T14:18:18.404424Z","shell.execute_reply.started":"2023-11-29T14:18:18.391473Z","shell.execute_reply":"2023-11-29T14:18:18.403664Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"##### Time to Train","metadata":{}},{"cell_type":"code","source":"lr = 1e-03\nepochs = 50\n\ndata = train(epochs, lr)","metadata":{"execution":{"iopub.status.busy":"2023-11-29T14:18:18.405418Z","iopub.execute_input":"2023-11-29T14:18:18.405735Z","iopub.status.idle":"2023-11-29T14:18:21.980464Z","shell.execute_reply.started":"2023-11-29T14:18:18.405704Z","shell.execute_reply":"2023-11-29T14:18:21.979103Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}